{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26661a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1c7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"./../feedback-grape\"))\n",
    "sys.path.append(os.path.abspath(\"./../\"))\n",
    "\n",
    "# ruff: noqa\n",
    "from feedback_grape.fgrape import optimize_pulse, evaluate_on_longer_time\n",
    "from helpers import (\n",
    "    init_grape_protocol,\n",
    "    init_fgrape_protocol,\n",
    "    test_implementations,\n",
    "    generate_random_state,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from library.utils.FgResult_to_dict import FgResult_to_dict\n",
    "import json\n",
    "\n",
    "test_implementations()\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95a59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical parameters\n",
    "# (attention! #elements in density matrix grow as 4^n*N_chains)\n",
    "n = 2 # number of qubits per chain (>= 2)\n",
    "N_chains = 2 # Number of parallel chains to simulate\n",
    "gamma = 0.25 # Decay constant\n",
    "\n",
    "assert n >= 2, \"Chain lengths must be at least 2.\"\n",
    "\n",
    "# Training and evaluation parameters\n",
    "training_params = {\n",
    "    \"N_samples\": 3, # Number of random initial states to sample\n",
    "    \"N_training_iterations\": 1000, # Number of training iterations\n",
    "    \"learning_rate\": 0.02, # Learning rate\n",
    "    \"convergence_threshold\": 1e-6,\n",
    "    \"batch_size\": 16,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"evaluation_time_steps\": 200,\n",
    "}\n",
    "\n",
    "# Parameters to test\n",
    "\n",
    "#num_time_steps : Number of time steps in the control pulse\n",
    "#lut_depth : Depth of the lookup table for feedback\n",
    "#reward_weights: Weights for the reward at each time step. Default only weights last timestep [0, 0, ... 0, 1]\n",
    "\n",
    "experiments = [ # (timesteps, lut_depth, reward_weights)\n",
    "    (1,1,[1]),\n",
    "    (2,1,[0,1]),\n",
    "    (2,1,[1,1]),\n",
    "    (3,1,[0,0,1]),\n",
    "    (3,1,[0,1,1]),\n",
    "    (3,1,[1,1,1]),\n",
    "\n",
    "    (1,1,[1]),\n",
    "    (2,2,[0,1]),\n",
    "    (2,2,[1,1]),\n",
    "    (3,3,[0,0,1]),\n",
    "    (3,3,[0,1,1]),\n",
    "    (3,3,[1,1,1]),\n",
    "]\n",
    "\n",
    "for t, l, weights in experiments:\n",
    "    assert type(t) == int and t >= 1, \"Number of time steps must be a positive integer.\"\n",
    "    assert type(l) == int and l >= 1, \"LUT depth must be a positive integer.\"\n",
    "    assert len(weights) == t, \"Length of weights must equal number of time steps.\"\n",
    "    assert l <= t, \"LUT depth cannot exceed number of time steps.\"\n",
    "    for w in weights:\n",
    "        assert type(w) == int and 0 <= w <= 9, \"Weights must be integers between 0 and 9 to save as single character in filename.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcac0ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating num_time_steps=1, lut_depth=1, reward_weights=[1]\n",
      "Training LUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:33<00:00, 11.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating num_time_steps=2, lut_depth=1, reward_weights=[0, 1]\n",
      "Training LUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Train LUT\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# Catch any errors during optimization, as we may sometimes encounter numerical issues\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     result = \u001b[43moptimize_pulse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mU_0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mC_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_time_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_time_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlut_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlut_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreward_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlookup\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgoal\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfidelity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mN_training_iterations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconvergence_threshold\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearning_rate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdensity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_batch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m best_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m result.final_fidelity > best_result.final_fidelity:\n\u001b[32m    108\u001b[39m         best_result = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/feedback-grape/feedback_grape/fgrape.py:809\u001b[39m, in \u001b[36moptimize_pulse\u001b[39m\u001b[34m(U_0, C_target, system_params, num_time_steps, max_iter, convergence_threshold, learning_rate, evo_type, lut_depth, reward_weights, goal, batch_size, eval_batch_size, mode, rnn, rnn_hidden_size, progress)\u001b[39m\n\u001b[32m    805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_sum1 + loss_sum2\n\u001b[32m    807\u001b[39m train_key, eval_key = jax.random.split(train_eval_key)\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m best_model_params, iter_idx = \u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    820\u001b[39m result = _evaluate(\n\u001b[32m    821\u001b[39m     U_0=U_0,\n\u001b[32m    822\u001b[39m     C_target=C_target,\n\u001b[32m   (...)\u001b[39m\u001b[32m    839\u001b[39m     num_iterations=iter_idx,\n\u001b[32m    840\u001b[39m )\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/feedback-grape/feedback_grape/fgrape.py:860\u001b[39m, in \u001b[36m_train\u001b[39m\u001b[34m(loss_fn, trainable_params, prng_key, max_iter, learning_rate, convergence_threshold, progress, early_stop)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[33;03mTrain the model using the specified optimizer.\u001b[39;00m\n\u001b[32m    857\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    858\u001b[39m \u001b[38;5;66;03m# Optimization\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# set up optimizer and training state\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m best_model_params, iter_idx = \u001b[43moptimize_adam_feedback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[38;5;66;03m# Due to the complex parameter l-bfgs is very slow and leads to bad results so is omitted\u001b[39;00m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_model_params, iter_idx\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/feedback-grape/feedback_grape/utils/optimizers.py:61\u001b[39m, in \u001b[36moptimize_adam_feedback\u001b[39m\u001b[34m(loss_fn, control_amplitudes, max_iter, learning_rate, convergence_threshold, key, progress, early_stop)\u001b[39m\n\u001b[32m     56\u001b[39m losses.append(loss)\n\u001b[32m     57\u001b[39m if early_stop:\n\u001b[32m     58\u001b[39m     if (\n\u001b[32m     59\u001b[39m         iter_idx > 0\n\u001b[32m     60\u001b[39m         and abs(losses[-1] - losses[-2]) < convergence_threshold\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     ):\n\u001b[32m     62\u001b[39m         break\n\u001b[32m     64\u001b[39m nan_detected = any([jax.numpy.any(jax.numpy.isnan(p)) for p in jax.tree_util.tree_leaves(new_params)])\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/pjit.py:341\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    337\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    340\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable,\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m  pgle_profiler) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    344\u001b[39m     executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr.effects,\n\u001b[32m    345\u001b[39m     jaxpr.consts, jit_info.abstracted_axes,\n\u001b[32m    346\u001b[39m     pgle_profiler)\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/pjit.py:195\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    194\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m   out_flat, compiled, profiler = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m   out_flat = pjit_p.bind(*args_flat, **p.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/pjit.py:1657\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1645\u001b[39m compiler_options_kvs = compiler_options_kvs + \u001b[38;5;28mtuple\u001b[39m(pgle_compile_options.items())\n\u001b[32m   1646\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1647\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m   1648\u001b[39m compiled = \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2446\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options)\u001b[39m\n\u001b[32m   2444\u001b[39m compiler_options_kvs = \u001b[38;5;28mself\u001b[39m._compiler_options_kvs + t_compiler_options\n\u001b[32m   2445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs:\n\u001b[32m-> \u001b[39m\u001b[32m2446\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2447\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2448\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2449\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2450\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2959\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2956\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   2958\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2959\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2965\u001b[39m orig_out_shardings = out_shardings\n\u001b[32m   2967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2757\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2749\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2750\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2751\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2752\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2754\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2755\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2756\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2757\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m      \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/compiler.py:473\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    472\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/compiler.py:690\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    682\u001b[39m     backend: xc.Client,\n\u001b[32m    683\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    687\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    688\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    689\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m   executable = \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    694\u001b[39m   _cache_write(\n\u001b[32m    695\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    696\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/profiler.py:334\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    333\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Forschungsprojekt III/code/.venv/lib/python3.13/site-packages/jax/_src/compiler.py:318\u001b[39m, in \u001b[36mbackend_compile\u001b[39m\u001b[34m(backend, module, options, host_callbacks)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    315\u001b[39m   \u001b[38;5;66;03m# we use a separate function call to ensure that XLA compilation appears\u001b[39;00m\n\u001b[32m    316\u001b[39m   \u001b[38;5;66;03m# separately in Python profiling results\u001b[39;00m\n\u001b[32m    317\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m host_callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    322\u001b[39m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    323\u001b[39m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m    324\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile(built_c, compile_options=options)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for num_time_steps, lut_depth, reward_weights in experiments:\n",
    "    print(f\"Evaluating num_time_steps={num_time_steps}, lut_depth={lut_depth}, reward_weights={reward_weights}\")\n",
    "    weights_str = \"\".join([str(w) for w in reward_weights])\n",
    "    state_callable = lambda key: generate_random_state(key, N_chains=N_chains)\n",
    "    # Skip Grape optimization since it is slow for num_time_steps > 2\n",
    "    \"\"\"\n",
    "    if os.path.exists(f\"./optimized_architectures/grape_t={num_time_steps}_s={training_params[\"N_samples\"]}_w={weights_str}.json\"):\n",
    "        print(f\"Grape for t={num_time_steps} and weights={weights_str} already optimized, skipping optimization.\")\n",
    "    else:\n",
    "        print(\"Optimizing grape\")\n",
    "\n",
    "        best_result = None\n",
    "        fidelities_each = []\n",
    "        for s in tqdm(range(training_params[\"N_samples\"])):\n",
    "            system_params = init_grape_protocol(jax.random.PRNGKey(s), n, N_chains, gamma)\n",
    "\n",
    "            # Optimize GRAPE\n",
    "            try: # Catch any errors during optimization, as we may sometimes encounter numerical issues\n",
    "                result = optimize_pulse(\n",
    "                    U_0=state_callable,\n",
    "                    C_target=state_callable,\n",
    "                    system_params=system_params,\n",
    "                    num_time_steps=num_time_steps,\n",
    "                    reward_weights=reward_weights,\n",
    "                    mode=\"no-measurement\",\n",
    "                    goal=\"fidelity\",\n",
    "                    max_iter=training_params[\"N_training_iterations\"],\n",
    "                    convergence_threshold=training_params[\"convergence_threshold\"],\n",
    "                    learning_rate=training_params[\"learning_rate\"],\n",
    "                    evo_type=\"density\",\n",
    "                    batch_size=training_params[\"batch_size\"],\n",
    "                    eval_batch_size=training_params[\"eval_batch_size\"],\n",
    "                )\n",
    "\n",
    "                if best_result is None or result.final_fidelity > best_result.final_fidelity:\n",
    "                    best_result = result\n",
    "\n",
    "                fidelities_each.append(float(result.final_fidelity))\n",
    "            except Exception as e:\n",
    "                if \"Argument rho0 is not hermitian.\" in str(e):\n",
    "                    print(f'Sample {s} failed with error: \"Argument rho0 is not hermitian.\"')\n",
    "                else:\n",
    "                    print(f\"Sample {s} failed with error: {e}\")\n",
    "                continue\n",
    "\n",
    "        if best_result is not None:\n",
    "            with open(f\"./optimized_architectures/grape_t={num_time_steps}_s={training_params[\"N_samples\"]}_w={weights_str}.json\", \"w\") as f:\n",
    "                json.dump(FgResult_to_dict(best_result), f)\n",
    "            \n",
    "            try:\n",
    "                result = evaluate_on_longer_time(\n",
    "                    U_0 = state_callable,\n",
    "                    C_target = state_callable,\n",
    "                    system_params = system_params,\n",
    "                    optimized_trainable_parameters = result.optimized_trainable_parameters,\n",
    "                    num_time_steps = training_params[\"evaluation_time_steps\"],\n",
    "                    evo_type = \"density\",\n",
    "                    goal = \"fidelity\",\n",
    "                    eval_batch_size = training_params[\"eval_batch_size\"],\n",
    "                    mode = \"no-measurement\",\n",
    "                )\n",
    "\n",
    "                fidelities_grape = result.fidelity_each_timestep\n",
    "\n",
    "                jnp.savez(f\"./evaluation_results/grape_t={num_time_steps}_s={training_params[\"N_samples\"]}_w={weights_str}.npz\", fidelities_grape=jnp.array(fidelities_grape))\n",
    "            except Exception as e:\n",
    "                if \"Argument rho0 is not hermitian.\" in str(e):\n",
    "                    print(f'Evaluation on longer time failed with error: \"Argument rho0 is not hermitian.\"')\n",
    "                else:\n",
    "                    print(f\"Evaluation on longer time failed with error: {e}\")\n",
    "\n",
    "        with open(f\"./optimized_architectures/grape_t={num_time_steps}_s={training_params[\"N_samples\"]}_w={weights_str}_training_data.json\", \"w\") as f:\n",
    "            training_data = {\n",
    "                \"fidelities_each_sample\": fidelities_each,\n",
    "                \"average_fidelity\": float(jnp.mean(jnp.array(fidelities_each))),\n",
    "                \"training_params\": training_params,\n",
    "            }\n",
    "            json.dump(training_data, f)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Training LUT\")\n",
    "\n",
    "    best_result = None\n",
    "    fidelities_each = []\n",
    "    for s in tqdm(range(training_params[\"N_samples\"])):\n",
    "        system_params = init_fgrape_protocol(jax.random.PRNGKey(s), n, N_chains, gamma)\n",
    "\n",
    "        # Train LUT\n",
    "        try: # Catch any errors during optimization, as we may sometimes encounter numerical issues\n",
    "            result = optimize_pulse(\n",
    "                U_0=state_callable,\n",
    "                C_target=state_callable,\n",
    "                system_params=system_params,\n",
    "                num_time_steps=num_time_steps,\n",
    "                lut_depth=lut_depth,\n",
    "                reward_weights=reward_weights,\n",
    "                mode=\"lookup\",\n",
    "                goal=\"fidelity\",\n",
    "                max_iter=training_params[\"N_training_iterations\"],\n",
    "                convergence_threshold=training_params[\"convergence_threshold\"],\n",
    "                learning_rate=training_params[\"learning_rate\"],\n",
    "                evo_type=\"density\",\n",
    "                batch_size=training_params[\"batch_size\"],\n",
    "                eval_batch_size=training_params[\"eval_batch_size\"],\n",
    "            )\n",
    "\n",
    "            if best_result is None or result.final_fidelity > best_result.final_fidelity:\n",
    "                best_result = result\n",
    "\n",
    "            fidelities_each.append(float(result.final_fidelity))\n",
    "        except Exception as e:\n",
    "            if \"Argument rho0 is not hermitian.\" in str(e):\n",
    "                print(f'Sample {s} failed with error: \"Argument rho0 is not hermitian.\"')\n",
    "            else:\n",
    "                print(f\"Sample {s} failed with error: {e}\")\n",
    "            continue\n",
    "\n",
    "    if best_result is not None:\n",
    "        with open(f\"./optimized_architectures/lut_t={num_time_steps}_l={lut_depth}_s={training_params[\"N_samples\"]}_w={weights_str}.json\", \"w\") as f:\n",
    "            json.dump(FgResult_to_dict(best_result), f)\n",
    "\n",
    "        try:\n",
    "            result = evaluate_on_longer_time(\n",
    "                U_0 = state_callable,\n",
    "                C_target = state_callable,\n",
    "                system_params = system_params,\n",
    "                optimized_trainable_parameters = result.optimized_trainable_parameters,\n",
    "                num_time_steps = training_params[\"evaluation_time_steps\"],\n",
    "                evo_type = \"density\",\n",
    "                goal = \"fidelity\",\n",
    "                eval_batch_size = training_params[\"eval_batch_size\"],\n",
    "                mode = \"lookup\",\n",
    "            )\n",
    "\n",
    "            fidelities_lut = result.fidelity_each_timestep\n",
    "\n",
    "            jnp.savez(f\"./evaluation_results/lut_t={num_time_steps}_l={lut_depth}_s={training_params[\"N_samples\"]}_w={weights_str}.npz\", fidelities_lut=jnp.array(fidelities_lut))\n",
    "        except Exception as e:\n",
    "            if \"Argument rho0 is not hermitian.\" in str(e):\n",
    "                print(f'Evaluation on longer time failed with error: \"Argument rho0 is not hermitian.\"')\n",
    "            else:\n",
    "                print(f\"Evaluation on longer time failed with error: {e}\")\n",
    "\n",
    "    with open(f\"./optimized_architectures/lut_t={num_time_steps}_l={lut_depth}_s={training_params[\"N_samples\"]}_w={weights_str}_training_data.json\", \"w\") as f:\n",
    "        training_data = {\n",
    "            \"fidelities_each_sample\": fidelities_each,\n",
    "            \"average_fidelity\": float(jnp.mean(jnp.array(fidelities_each))),\n",
    "            \"training_params\": training_params,\n",
    "        }\n",
    "        json.dump(training_data, f)\n",
    "    \n",
    "    \n",
    "    # Check if RNN has to be trained (only if num_time_steps and weight are different)\n",
    "    if os.path.exists(f\"./optimized_architectures/rnn_t={num_time_steps}_s={training_params[\"N_samples\"]}_w={weights_str}.json\"):\n",
    "        print(f\"RNN for t={num_time_steps} and weights={weights_str} already trained, skipping training.\")\n",
    "    else:\n",
    "        print(\"Training RNN\")\n",
    "\n",
    "        best_result = None\n",
    "        fidelities_each = []\n",
    "        for s in tqdm(range(training_params[\"N_samples\"])):\n",
    "            system_params = init_fgrape_protocol(jax.random.PRNGKey(s), n, N_chains, gamma)\n",
    "\n",
    "            # Train RNN\n",
    "            try: # Catch any errors during optimization, as we may sometimes encounter numerical issues\n",
    "                result = optimize_pulse(\n",
    "                    U_0=state_callable,\n",
    "                    C_target=state_callable,\n",
    "                    system_params=system_params,\n",
    "                    num_time_steps=num_time_steps,\n",
    "                    lut_depth=lut_depth,\n",
    "                    reward_weights=reward_weights,\n",
    "                    mode=\"nn\",\n",
    "                    goal=\"fidelity\",\n",
    "                    max_iter=training_params[\"N_training_iterations\"],\n",
    "                    convergence_threshold=training_params[\"convergence_threshold\"],\n",
    "                    learning_rate=training_params[\"learning_rate\"],\n",
    "                    evo_type=\"density\",\n",
    "                    batch_size=training_params[\"batch_size\"],\n",
    "                    eval_batch_size=training_params[\"eval_batch_size\"],\n",
    "                )\n",
    "\n",
    "                if best_result is None or result.final_fidelity > best_result.final_fidelity:\n",
    "                    best_result = result\n",
    "\n",
    "                fidelities_each.append(float(result.final_fidelity))\n",
    "            except Exception as e:\n",
    "                if \"Argument rho0 is not hermitian.\" in str(e):\n",
    "                    print(f'Sample {s} failed with error: \"Argument rho0 is not hermitian.\"')\n",
    "                else:\n",
    "                    print(f\"Sample {s} failed with error: {e}\")\n",
    "                continue\n",
    "\n",
    "        if best_result is not None:\n",
    "            with open(f\"./optimized_architectures/rnn_t={num_time_steps}_s={training_params[\"N_samples\"]}_w={weights_str}.json\", \"w\") as f:\n",
    "                json.dump(FgResult_to_dict(best_result), f)\n",
    "\n",
    "            try:\n",
    "                result = evaluate_on_longer_time(\n",
    "                    U_0 = state_callable,\n",
    "                    C_target = state_callable,\n",
    "                    system_params = system_params,\n",
    "                    optimized_trainable_parameters = result.optimized_trainable_parameters,\n",
    "                    num_time_steps = training_params[\"evaluation_time_steps\"],\n",
    "                    evo_type = \"density\",\n",
    "                    goal = \"fidelity\",\n",
    "                    eval_batch_size = training_params[\"eval_batch_size\"],\n",
    "                    mode = \"nn\",\n",
    "                )\n",
    "\n",
    "                fidelities_rnn = result.fidelity_each_timestep\n",
    "\n",
    "                jnp.savez(f\"./evaluation_results/rnn_t={num_time_steps}_s={training_params[\"N_samples\"]}_w={weights_str}.npz\", fidelities_rnn=jnp.array(fidelities_rnn))\n",
    "            except Exception as e:\n",
    "                if \"Argument rho0 is not hermitian.\" in str(e):\n",
    "                    print(f'Evaluation on longer time failed with error: \"Argument rho0 is not hermitian.\"')\n",
    "                else:\n",
    "                    print(f\"Evaluation on longer time failed with error: {e}\")\n",
    "\n",
    "        with open(f\"./optimized_architectures/rnn_t={num_time_steps}_s={training_params[\"N_samples\"]}_w={weights_str}_training_data.json\", \"w\") as f:\n",
    "            training_data = {\n",
    "                \"fidelities_each_sample\": fidelities_each,\n",
    "                \"average_fidelity\": float(jnp.mean(jnp.array(fidelities_each))),\n",
    "                \"training_params\": training_params,\n",
    "            }\n",
    "            json.dump(training_data, f)\n",
    "    \n",
    "# Play a sound when done\n",
    "os.system('say \"done.\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
