{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1c7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"./../../feedback-grape\"))\n",
    "sys.path.append(os.path.abspath(\"./../\"))\n",
    "sys.path.append(os.path.abspath(\"./../../\"))\n",
    "\n",
    "# ruff: noqa\n",
    "from feedback_grape.fgrape import optimize_pulse, evaluate_on_longer_time\n",
    "from helpers import (\n",
    "    init_grape_protocol,\n",
    "    init_fgrape_protocol,\n",
    "    test_implementations,\n",
    "    generate_random_discrete_state,\n",
    "    generate_random_bloch_state\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from library.utils.FgResult_to_dict import FgResult_to_dict\n",
    "import json\n",
    "\n",
    "test_implementations()\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95a59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical parameters\n",
    "# (attention! #elements in density matrix grow as 4^N_chains)\n",
    "N_chains = 3 # Number of parallel chains to simulate\n",
    "gamma = 0.05 # Decay constant\n",
    "\n",
    "# Training and evaluation parameters\n",
    "training_params = {\n",
    "    \"N_samples\": 10, # Number of random initial states to sample\n",
    "    \"N_training_iterations\": 10000, # Number of training iterations\n",
    "    \"learning_rate\": 0.001, # Learning rate\n",
    "    \"convergence_threshold\": 1e-6,\n",
    "    \"batch_size\": 8,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"evaluation_time_steps\": 100,\n",
    "}\n",
    "N_parallel_threads = 4 # Number of parallel threads for training\n",
    "generate_state = generate_random_bloch_state\n",
    "\n",
    "# Parameters to test\n",
    "\n",
    "#num_time_steps : Number of time steps in the control pulse\n",
    "#lut_depth : Depth of the lookup table for feedback\n",
    "#reward_weights: Weights for the reward at each time step. Default only weights last timestep [0, 0, ... 0, 1]\n",
    "\n",
    "experiments = [ # (timesteps, lut_depth, reward_weights, noise_level)\n",
    "    (6,2,[1,1,1,1,1,1], 0.0),\n",
    "    (6,2,[1,1,1,1,1,1], 0.5),\n",
    "    (6,2,[1,1,1,1,1,1], 1.0),\n",
    "    (6,2,[1,1,1,1,1,1], float('inf')),\n",
    "    (7,2,[1,1,1,1,1,1,1], 0.0),\n",
    "    (7,2,[1,1,1,1,1,1,1], 0.5),\n",
    "    (7,2,[1,1,1,1,1,1,1], 1.0),\n",
    "    (7,2,[1,1,1,1,1,1,1], float('inf')),\n",
    "]\n",
    "\n",
    "experiments_format = [ # Format of variables as tuples (name, type, required by grape, required by lut, required by rnn)\n",
    "    (\"t\", int, True, True, True), # number timesteps (or segments)\n",
    "    (\"l\", int, False, True, False), # LUT depth\n",
    "    (\"w\", list, True, True, True), # reward weights\n",
    "    (\"noise\", float, True, True, True), # noise level\n",
    "]\n",
    "\n",
    "# Check if experiment parameters are in specified format\n",
    "for params in experiments:\n",
    "    assert len(params) == len(experiments_format), \"Experiment parameters length mismatch.\"\n",
    "    for i, (param, (name, expected_type, req_grape, req_lut, req_rnn)) in enumerate(zip(params, experiments_format)):\n",
    "        assert type(name) == str, \"Parameter name must be a string.\"\n",
    "        assert type(req_grape) == bool, \"Requirement flags must be boolean.\"\n",
    "        assert type(req_lut) == bool, \"Requirement flags must be boolean.\"\n",
    "        assert type(req_rnn) == bool, \"Requirement flags must be boolean.\"\n",
    "        assert type(param) == expected_type, f\"Parameter '{name}' at index {i} must be of type {expected_type.__name__}.\"\n",
    "        if type(param) == list:\n",
    "            for j, item in enumerate(param):\n",
    "                assert type(item) == int and 0 <= item <= 9, f\"Item at index {j} in parameter '{name}' must be of type int and between 0 and 9.\"\n",
    "\n",
    "def generate_param_paths(params, model_type):\n",
    "    assert model_type in [\"grape\", \"lut\", \"rnn\"], \"Invalid model type.\"\n",
    "\n",
    "    parts = []\n",
    "    for (name, expected_type, req_grape, req_lut, req_rnn), param in zip(experiments_format, params):\n",
    "        required = (model_type == \"grape\" and req_grape) or (model_type == \"lut\" and req_lut) or (model_type == \"rnn\" and req_rnn)\n",
    "        if required:\n",
    "            if expected_type == list:\n",
    "                param_str = \"\".join(str(x) for x in param)\n",
    "            else:\n",
    "                param_str = str(param)\n",
    "            parts.append(f\"{name}={param_str}\")\n",
    "\n",
    "    pstr = f\"{model_type}_\" + \"_\".join(parts)\n",
    "    return (\n",
    "        \"./optimized_architectures/\" + pstr + \".json\",\n",
    "        \"./optimized_architectures/\" + pstr + \"_training_data.json\",\n",
    "        \"./evaluation_results/\" + pstr + \".npz\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcac0ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating (6, 2, [1, 1, 1, 1, 1, 1], 0.0)Evaluating (6, 2, [1, 1, 1, 1, 1, 1], 0.5)\n",
      "Evaluating (6, 2, [1, 1, 1, 1, 1, 1], 1.0)\n",
      "Training LUT\n",
      "Training LUT\n",
      "\n",
      "Training LUT\n",
      "Evaluating (6, 2, [1, 1, 1, 1, 1, 1], inf)\n",
      "Training LUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      " 10%|█         | 1/10 [05:07<46:11, 307.96s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      " 20%|██        | 2/10 [09:59<39:45, 298.25s/it]\n",
      "\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [16:03<38:19, 328.47s/it]\n",
      "\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [21:43<33:18, 333.05s/it]\n",
      "\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [27:18<27:47, 333.53s/it]\n",
      "\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [33:47<23:29, 352.37s/it]\n",
      "\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [40:04<18:01, 360.52s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A2025-10-30 16:28:27.924667: E external/xla/xla/service/slow_operation_alarm.cc:73] \n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      " 80%|████████  | 8/10 [48:08<13:19, 399.87s/it]2025-10-30 16:33:23.046511: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 6m55.129933s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2025-10-30 16:35:37.231445: E external/xla/xla/service/slow_operation_alarm.cc:73] \n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2025-10-30 16:39:08.348547: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 5m31.12166s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "\n",
      "\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [1:01:31<08:45, 525.89s/it]\n",
      "\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [1:06:14<00:00, 397.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating (7, 2, [1, 1, 1, 1, 1, 1, 1], 0.0)\n",
      "Training LUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:07:39<00:00, 405.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating (7, 2, [1, 1, 1, 1, 1, 1, 1], 0.5)\n",
      "Training LUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 10/10 [1:09:02<00:00, 414.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating (7, 2, [1, 1, 1, 1, 1, 1, 1], 1.0)\n",
      "Training LUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [1:10:38<00:00, 423.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating (7, 2, [1, 1, 1, 1, 1, 1, 1], inf)\n",
      "Training LUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 1/10 [10:21<1:33:17, 621.90s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A2025-10-30 17:07:10.770266: E external/xla/xla/service/slow_operation_alarm.cc:73] \n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2025-10-30 17:08:49.472204: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 3m38.702445s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2025-10-30 17:09:42.436775: E external/xla/xla/service/slow_operation_alarm.cc:73] \n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2025-10-30 17:10:26.061820: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 2m43.633551s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      " 20%|██        | 2/10 [20:32<1:22:03, 615.40s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def run_experiment(params):\n",
    "    num_time_steps, lut_depth, reward_weights, noise_level = params\n",
    "    print(f\"Evaluating {params}\")\n",
    "    noisy_state = lambda key: generate_state(key, N_chains=N_chains, noise_level=noise_level)\n",
    "    pure_state = lambda key: generate_state(key, N_chains=N_chains, noise_level=0.0)\n",
    "\n",
    "    # Only test LUTs in this example\n",
    "    path1, path2, path3 = generate_param_paths(params, \"lut\")\n",
    "    if os.path.exists(path1):\n",
    "        print(f\"LUT already trained, skipping training.\")\n",
    "    else:\n",
    "        print(\"Training LUT\")\n",
    "\n",
    "        best_result = None\n",
    "        best_result_fidelities = None\n",
    "        fidelities_each = []\n",
    "        for s in tqdm(range(training_params[\"N_samples\"])):\n",
    "            system_params = init_fgrape_protocol(jax.random.PRNGKey(s), N_chains, gamma)\n",
    "\n",
    "            # Train LUT\n",
    "            result = optimize_pulse(\n",
    "                U_0=noisy_state,\n",
    "                C_target=pure_state,\n",
    "                system_params=system_params,\n",
    "                num_time_steps=num_time_steps,\n",
    "                lut_depth=lut_depth,\n",
    "                reward_weights=reward_weights,\n",
    "                mode=\"lookup\",\n",
    "                goal=\"fidelity\",\n",
    "                max_iter=training_params[\"N_training_iterations\"],\n",
    "                convergence_threshold=training_params[\"convergence_threshold\"],\n",
    "                learning_rate=training_params[\"learning_rate\"],\n",
    "                evo_type=\"density\",\n",
    "                batch_size=training_params[\"batch_size\"],\n",
    "                eval_batch_size=1, # This evaluation is discarded\n",
    "            )\n",
    "\n",
    "            eval_result = evaluate_on_longer_time( # Evaluate on longer time and choose best LUT accordingly\n",
    "                U_0 = pure_state,\n",
    "                C_target = pure_state,\n",
    "                system_params = system_params,\n",
    "                optimized_trainable_parameters = result.optimized_trainable_parameters,\n",
    "                num_time_steps = training_params[\"evaluation_time_steps\"],\n",
    "                evo_type = \"density\",\n",
    "                goal = \"fidelity\",\n",
    "                eval_batch_size = training_params[\"eval_batch_size\"],\n",
    "                mode = \"lookup\",\n",
    "            )\n",
    "\n",
    "            fidelities = eval_result.fidelity_each_timestep\n",
    "\n",
    "            if best_result is None or np.mean(fidelities) > np.mean(best_result_fidelities):\n",
    "                best_result = result\n",
    "                best_result_fidelities = fidelities\n",
    "\n",
    "            fidelities_each.append(float(np.mean(fidelities)))\n",
    "\n",
    "        np.savez(path3, fidelities_lut=np.array(best_result_fidelities))\n",
    "\n",
    "        with open(path1, \"w\") as f:\n",
    "            json.dump(FgResult_to_dict(best_result), f)\n",
    "\n",
    "        with open(path2, \"w\") as f:\n",
    "            training_data = {\n",
    "                \"fidelities_each_sample\": fidelities_each,\n",
    "                \"average_fidelity\": float(np.mean(fidelities_each)),\n",
    "                \"training_params\": training_params,\n",
    "            }\n",
    "            json.dump(training_data, f)\n",
    "\n",
    "# Run experiments in parallel\n",
    "if N_parallel_threads > 1:\n",
    "    with ThreadPoolExecutor(max_workers=N_parallel_threads) as executor:\n",
    "        executor.map(run_experiment, experiments)\n",
    "else:\n",
    "    for params in experiments:\n",
    "        run_experiment(params)\n",
    "\n",
    "# Play a sound when done\n",
    "os.system('say \"done.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06921e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
