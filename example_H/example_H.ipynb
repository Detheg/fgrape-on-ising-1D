{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16070338",
   "metadata": {},
   "source": [
    "# E: State stabilization with SNAP gates and displacement gates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e400e",
   "metadata": {},
   "source": [
    "The use of feedback GRAPE applied to the Jaynes-\n",
    "Cummings scenario allows us to discover strategies\n",
    "extending the lifetime of a range of quantum states. How-\n",
    "ever, for more complex quantum states such as kitten\n",
    "states, the infidelity becomes significant after just a few\n",
    "dissipative evolution steps in spite of the feedback [cf.\n",
    "Fig. 6(c)]. This raises the question of whether the limited\n",
    "quality of the stabilization is to be attributed to a failure\n",
    "of our feedback-GRAPE learning algorithm to properly\n",
    "explore the control-parameter landscape or, rather, to the\n",
    "limited expressivity of the controls. With the goal of\n",
    "addressing this question, we test our method on the state-\n",
    "stabilization task using a more expressive control scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6003222d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"./../feedback-grape\"))\n",
    "sys.path.append(os.path.abspath(\"./../\"))\n",
    "\n",
    "# ruff: noqa\n",
    "from feedback_grape.fgrape import optimize_pulse, Decay, Gate, evaluate_on_longer_time\n",
    "from feedback_grape.utils.operators import cosm, sinm, identity\n",
    "from feedback_grape.utils.states import coherent\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from feedback_grape.utils.operators import create, destroy\n",
    "from feedback_grape.utils.fidelity import ket2dm\n",
    "from library.utils.FgResult_to_dict import FgResult_to_dict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851f72b",
   "metadata": {},
   "source": [
    "## Initialize states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa672e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "N_training_iterations = 1000 # Number of training iterations\n",
    "learning_rate = 0.01 # Learning rate\n",
    "convergence_threshold = 1e-6 # Convergence threshold for early stopping\n",
    "\n",
    "#num_time_steps : Number of time steps in the control pulse\n",
    "#lut_depth : Depth of the lookup table for feedback\n",
    "#reward_weights: Weights for the reward at each time step. Default only weights last timestep [0, 0, ... 0, 1]\n",
    "\n",
    "experiments = [ # (timesteps, lut_depth, reward_weights)\n",
    "    #(1,1,[1]),\n",
    "    #(2,1,[1,1]),\n",
    "    #(2,2,[0,1]),\n",
    "    #(2,2,[1,1]),\n",
    "    #(3,3,[0,0,1]),\n",
    "    (3,3,[1,1,1]),\n",
    "    #(4,4,[0,0,0,1]),\n",
    "    #(4,3,[0,0,0,1]),\n",
    "    #(4,3,[0,0,1,1]),\n",
    "    #(4,3,[1,1,1,1]),\n",
    "    #(4,4,[1,1,1,1]),\n",
    "]\n",
    "\n",
    "# Physical parameters\n",
    "N_cav = 30  # number of cavity modes\n",
    "N_snap = 15\n",
    "\n",
    "alpha = 2\n",
    "psi_target = coherent(N_cav, alpha) + coherent(N_cav, -alpha)\n",
    "\n",
    "# Normalize psi_target before constructing rho_target\n",
    "psi_target = psi_target / jnp.linalg.norm(psi_target)\n",
    "\n",
    "rho_target = ket2dm(psi_target)\n",
    "\n",
    "for _, _, weights in experiments:\n",
    "    for w in weights:\n",
    "        assert type(w) == int and 0 <= w <= 9, \"Weights must be integers between 0 and 9 to save as single character in filename.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda36a",
   "metadata": {},
   "source": [
    "## Initialize the parameterized Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9381af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displacement_gate(alphas):\n",
    "    \"\"\"Displacement operator for a coherent state.\"\"\"\n",
    "    alpha_re, alpha_im = alphas\n",
    "    alpha = alpha_re + 1j * alpha_im\n",
    "    gate = jax.scipy.linalg.expm(\n",
    "        alpha * create(N_cav) - alpha.conj() * destroy(N_cav)\n",
    "    )\n",
    "    return gate\n",
    "\n",
    "def initialize_displacement_gate(key):\n",
    "    return Gate(\n",
    "        gate=displacement_gate,\n",
    "        initial_params=jax.random.uniform(\n",
    "            key,\n",
    "            shape=(2,),\n",
    "            minval=-jnp.pi / 2,\n",
    "            maxval=jnp.pi / 2,\n",
    "            dtype=jnp.float64,\n",
    "        ),\n",
    "        measurement_flag=False,\n",
    "    )\n",
    "\n",
    "def displacement_gate_dag(alphas):\n",
    "    \"\"\"Displacement operator for a coherent state.\"\"\"\n",
    "    return displacement_gate(alphas).conj().T\n",
    "\n",
    "def initialize_displacement_gate_dag(key):\n",
    "    return Gate(\n",
    "        gate=displacement_gate_dag,\n",
    "        initial_params=jax.random.uniform(\n",
    "            key,\n",
    "            shape=(2,),\n",
    "            minval=-jnp.pi / 2,\n",
    "            maxval=jnp.pi / 2,\n",
    "            dtype=jnp.float64,\n",
    "        ),\n",
    "        measurement_flag=False,\n",
    "    )\n",
    "\n",
    "def snap_gate(phase_list):\n",
    "    diags = jnp.ones(shape=(N_cav - len(phase_list)))\n",
    "    exponentiated = jnp.exp(1j * jnp.array(phase_list))\n",
    "    diags = jnp.concatenate((exponentiated, diags))\n",
    "    return jnp.diag(diags)\n",
    "\n",
    "def initialize_snap_gate(key):\n",
    "    return Gate(\n",
    "        gate=snap_gate,\n",
    "        initial_params=jax.random.uniform(\n",
    "            key,\n",
    "            shape=(N_snap,),\n",
    "            minval=-jnp.pi / 2,\n",
    "            maxval=jnp.pi / 2,\n",
    "            dtype=jnp.float64,\n",
    "        ),\n",
    "        measurement_flag=False,\n",
    "    )\n",
    "\n",
    "def povm_measure_operator(measurement_outcome, params):\n",
    "    \"\"\"\n",
    "    POVM for the measurement of the cavity state.\n",
    "    returns Mm ( NOT the POVM element Em = Mm_dag @ Mm ), given measurement_outcome m, gamma and delta\n",
    "    \"\"\"\n",
    "    gamma, delta = params\n",
    "    cav_operator = gamma * create(N_cav) @ destroy(N_cav) + delta * identity(N_cav) / 2\n",
    "    angle = cav_operator\n",
    "    meas_op = jnp.where(\n",
    "        measurement_outcome == 1,\n",
    "        cosm(angle),\n",
    "        sinm(angle),\n",
    "    )\n",
    "    return meas_op\n",
    "\n",
    "def initialize_povm_gate(key):\n",
    "    return Gate(\n",
    "        gate=povm_measure_operator,\n",
    "        initial_params=jax.random.uniform(\n",
    "            key,\n",
    "            shape=(2,),  # 2 for gamma and delta\n",
    "            minval=-jnp.pi / 2,\n",
    "            maxval=jnp.pi / 2,\n",
    "            dtype=jnp.float64,\n",
    "        ),\n",
    "        measurement_flag=True,\n",
    "    )\n",
    "\n",
    "decay_gate = Decay(c_ops=[jnp.sqrt(0.005) * destroy(N_cav)])\n",
    "\n",
    "def initialize_system_params(key):\n",
    "    keys = jax.random.split(key, 4)\n",
    "    return [\n",
    "        decay_gate,\n",
    "        initialize_povm_gate(keys[0]),\n",
    "        decay_gate,\n",
    "        initialize_displacement_gate(keys[1]),\n",
    "        initialize_snap_gate(keys[2]),\n",
    "        initialize_displacement_gate_dag(keys[3])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d72e6",
   "metadata": {},
   "source": [
    "## Initialize RNN of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83584086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "# You can do whatever you want inside so long as you maintaing the hidden_size and output size shapes\n",
    "class RNN(nn.Module):\n",
    "    hidden_size: int  # number of features in the hidden state\n",
    "    output_size: int  # number of features in the output (inferred from the number of parameters) just provide those attributes to the class\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, measurement, hidden_state):\n",
    "\n",
    "        if measurement.ndim == 1:\n",
    "            measurement = measurement.reshape(1, -1)\n",
    "\n",
    "        ###############\n",
    "        ### Free to change whatever you want below as long as hidden layers have size self.hidden_size\n",
    "        ### and output layer has size self.output_size\n",
    "        ###############\n",
    "\n",
    "        gru_cell = nn.GRUCell(\n",
    "            features=self.hidden_size,\n",
    "            gate_fn=nn.sigmoid,\n",
    "            activation_fn=nn.tanh,\n",
    "        )\n",
    "        self.make_rng('dropout')\n",
    "\n",
    "        new_hidden_state, _ = gru_cell(hidden_state, measurement)\n",
    "        new_hidden_state = nn.Dropout(rate=0.1, deterministic=False)(\n",
    "            new_hidden_state\n",
    "        )\n",
    "        # this returns the povm_params after linear regression through the hidden state which contains\n",
    "        # the information of the previous time steps and this is optimized to output best povm_params\n",
    "        # new_hidden_state = nn.Dense(features=self.hidden_size)(new_hidden_state)\n",
    "        new_hidden_state = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "        )(new_hidden_state)\n",
    "        new_hidden_state = nn.relu(new_hidden_state)\n",
    "        new_hidden_state = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "        )(new_hidden_state)\n",
    "        new_hidden_state = nn.relu(new_hidden_state)\n",
    "        output = nn.Dense(\n",
    "            features=self.output_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "            bias_init=nn.initializers.constant(0.1),\n",
    "        )(new_hidden_state)\n",
    "        output = nn.relu(output)\n",
    "\n",
    "        ###############\n",
    "        ### Do not change the return statement\n",
    "        ###############\n",
    "\n",
    "        return output[0], new_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b486ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating num_time_steps=3, lut_depth=3, reward_weights=[1, 1, 1]\n",
      "Training LUT\n",
      "Iteration 10, Loss: 0.075175, T=9s, eta=842s\n",
      "Iteration 20, Loss: 0.107546, T=18s, eta=860s\n",
      "Iteration 30, Loss: 0.203866, T=27s, eta=873s\n",
      "Iteration 40, Loss: 0.155899, T=37s, eta=885s\n",
      "Iteration 50, Loss: 0.222191, T=46s, eta=874s\n",
      "Iteration 60, Loss: 0.200501, T=56s, eta=872s\n",
      "Iteration 70, Loss: 0.179746, T=67s, eta=885s\n",
      "Iteration 80, Loss: 0.189495, T=77s, eta=882s\n",
      "Iteration 90, Loss: 0.120069, T=87s, eta=880s\n",
      "Iteration 100, Loss: 0.168767, T=97s, eta=868s\n",
      "Iteration 110, Loss: 0.153255, T=106s, eta=855s\n",
      "Iteration 120, Loss: 0.162103, T=115s, eta=843s\n",
      "Iteration 130, Loss: 0.167811, T=126s, eta=839s\n",
      "Iteration 140, Loss: 0.117501, T=136s, eta=832s\n",
      "Iteration 150, Loss: 0.079862, T=145s, eta=818s\n",
      "Iteration 160, Loss: -0.069692, T=154s, eta=805s\n",
      "Iteration 170, Loss: -0.081731, T=163s, eta=794s\n",
      "Iteration 180, Loss: 0.074048, T=174s, eta=790s\n",
      "Iteration 190, Loss: -0.069830, T=183s, eta=781s\n",
      "Iteration 200, Loss: -0.253434, T=193s, eta=770s\n",
      "Iteration 210, Loss: -0.226104, T=202s, eta=757s\n",
      "Iteration 220, Loss: -0.706807, T=210s, eta=745s\n",
      "Iteration 230, Loss: -0.509834, T=219s, eta=733s\n",
      "Iteration 240, Loss: -1.052782, T=228s, eta=722s\n",
      "Iteration 250, Loss: -1.613074, T=238s, eta=713s\n",
      "Iteration 260, Loss: -1.577992, T=247s, eta=703s\n",
      "Iteration 270, Loss: -2.233187, T=256s, eta=691s\n",
      "Iteration 280, Loss: -1.690641, T=265s, eta=680s\n",
      "Iteration 290, Loss: -1.604927, T=274s, eta=671s\n",
      "Iteration 300, Loss: -1.538703, T=283s, eta=660s\n",
      "Iteration 310, Loss: -2.382971, T=293s, eta=651s\n",
      "Iteration 320, Loss: -2.298895, T=302s, eta=640s\n",
      "Iteration 330, Loss: -2.055480, T=310s, eta=630s\n",
      "Iteration 340, Loss: -1.637012, T=319s, eta=619s\n",
      "Iteration 350, Loss: -2.230478, T=328s, eta=609s\n",
      "Iteration 360, Loss: -2.232283, T=338s, eta=600s\n",
      "Iteration 370, Loss: -2.438981, T=347s, eta=590s\n",
      "Iteration 380, Loss: -1.949009, T=356s, eta=580s\n",
      "Iteration 390, Loss: -1.920109, T=367s, eta=573s\n",
      "Iteration 400, Loss: -2.541380, T=376s, eta=563s\n",
      "Iteration 410, Loss: -2.237329, T=385s, eta=553s\n",
      "Iteration 420, Loss: -2.548781, T=394s, eta=544s\n",
      "Iteration 430, Loss: -2.445164, T=403s, eta=534s\n",
      "Iteration 440, Loss: -2.001168, T=412s, eta=524s\n",
      "Iteration 450, Loss: -2.445627, T=421s, eta=514s\n",
      "Iteration 460, Loss: -2.119695, T=430s, eta=505s\n",
      "Iteration 470, Loss: -2.222939, T=439s, eta=495s\n",
      "Iteration 480, Loss: -2.000288, T=448s, eta=485s\n",
      "Iteration 490, Loss: -2.089174, T=457s, eta=476s\n",
      "Iteration 500, Loss: -2.011990, T=466s, eta=466s\n",
      "Iteration 510, Loss: -1.472366, T=476s, eta=457s\n",
      "Iteration 520, Loss: -2.316985, T=485s, eta=447s\n",
      "Iteration 530, Loss: -1.948020, T=494s, eta=438s\n",
      "Iteration 540, Loss: -2.198727, T=503s, eta=428s\n",
      "Iteration 550, Loss: -2.447320, T=512s, eta=419s\n",
      "Iteration 560, Loss: -2.103755, T=521s, eta=409s\n",
      "Iteration 570, Loss: -1.817601, T=530s, eta=400s\n",
      "Iteration 580, Loss: -2.209124, T=539s, eta=390s\n",
      "Iteration 590, Loss: -1.951694, T=548s, eta=381s\n",
      "Iteration 600, Loss: -2.326535, T=557s, eta=371s\n",
      "Iteration 610, Loss: -2.070890, T=566s, eta=362s\n",
      "Iteration 620, Loss: -0.569922, T=575s, eta=353s\n",
      "Iteration 630, Loss: -2.528685, T=584s, eta=343s\n",
      "Iteration 640, Loss: -2.292416, T=593s, eta=334s\n",
      "Iteration 650, Loss: -2.168842, T=602s, eta=324s\n",
      "Iteration 660, Loss: -2.170776, T=611s, eta=315s\n",
      "Iteration 670, Loss: -1.341264, T=620s, eta=306s\n",
      "Iteration 680, Loss: -2.012351, T=629s, eta=296s\n",
      "Iteration 690, Loss: -2.202659, T=638s, eta=287s\n",
      "Iteration 700, Loss: -2.447264, T=647s, eta=278s\n",
      "Iteration 710, Loss: -2.161727, T=656s, eta=268s\n",
      "Iteration 720, Loss: -2.552432, T=665s, eta=259s\n",
      "Iteration 730, Loss: -1.509321, T=674s, eta=250s\n",
      "Iteration 740, Loss: -2.041298, T=683s, eta=240s\n",
      "Iteration 750, Loss: -2.049490, T=692s, eta=231s\n",
      "Iteration 760, Loss: -2.558616, T=701s, eta=222s\n",
      "Iteration 770, Loss: -2.436779, T=709s, eta=212s\n",
      "Iteration 780, Loss: -2.024864, T=718s, eta=203s\n",
      "Iteration 790, Loss: -1.393384, T=727s, eta=194s\n",
      "Iteration 800, Loss: -1.906526, T=736s, eta=184s\n",
      "Iteration 810, Loss: -2.276676, T=745s, eta=175s\n",
      "Iteration 820, Loss: -2.161204, T=754s, eta=166s\n",
      "Iteration 830, Loss: -2.076054, T=762s, eta=156s\n",
      "Iteration 840, Loss: -2.159715, T=771s, eta=147s\n",
      "Iteration 850, Loss: -2.188160, T=780s, eta=138s\n",
      "Iteration 860, Loss: -0.562615, T=789s, eta=129s\n",
      "Iteration 870, Loss: -1.627992, T=798s, eta=120s\n",
      "Iteration 880, Loss: -1.792590, T=806s, eta=110s\n",
      "Iteration 890, Loss: -1.121475, T=815s, eta=101s\n",
      "Iteration 900, Loss: -1.510998, T=824s, eta=92s\n",
      "Iteration 910, Loss: -1.186649, T=833s, eta=83s\n",
      "Iteration 920, Loss: -1.695933, T=842s, eta=74s\n",
      "Iteration 930, Loss: -2.562617, T=851s, eta=64s\n",
      "Iteration 940, Loss: -1.335538, T=859s, eta=55s\n",
      "Iteration 950, Loss: -2.289044, T=868s, eta=46s\n",
      "Iteration 960, Loss: -1.519860, T=877s, eta=37s\n",
      "Iteration 970, Loss: -2.328883, T=886s, eta=28s\n",
      "Iteration 980, Loss: -2.179308, T=895s, eta=19s\n",
      "Iteration 990, Loss: -2.162702, T=903s, eta=10s\n",
      "Training RNN\n",
      "Iteration 10, Loss: -0.164790, T=10s, eta=915s\n",
      "Iteration 20, Loss: -0.941998, T=20s, eta=952s\n",
      "Iteration 30, Loss: -1.114587, T=29s, eta=930s\n",
      "Iteration 40, Loss: -1.017279, T=38s, eta=911s\n",
      "Iteration 50, Loss: -2.408870, T=48s, eta=898s\n",
      "Iteration 60, Loss: -2.654793, T=57s, eta=883s\n",
      "Iteration 70, Loss: -2.733768, T=66s, eta=870s\n",
      "Iteration 80, Loss: -2.733088, T=76s, eta=873s\n",
      "Iteration 90, Loss: -2.733344, T=86s, eta=865s\n",
      "Iteration 100, Loss: -2.045724, T=96s, eta=861s\n",
      "Iteration 110, Loss: -2.033581, T=105s, eta=850s\n",
      "Iteration 120, Loss: -2.765498, T=116s, eta=845s\n",
      "Iteration 130, Loss: -2.759144, T=125s, eta=832s\n",
      "Iteration 140, Loss: -2.755142, T=136s, eta=830s\n",
      "Iteration 150, Loss: -2.750044, T=146s, eta=823s\n",
      "Iteration 160, Loss: -2.758116, T=155s, eta=813s\n",
      "Iteration 170, Loss: -2.767077, T=165s, eta=805s\n",
      "Iteration 180, Loss: -2.765720, T=175s, eta=796s\n",
      "Iteration 190, Loss: -2.769637, T=189s, eta=805s\n",
      "Iteration 200, Loss: -2.762141, T=199s, eta=795s\n",
      "Iteration 210, Loss: -2.287905, T=211s, eta=791s\n",
      "Iteration 220, Loss: -2.767259, T=227s, eta=802s\n",
      "Iteration 230, Loss: -2.771222, T=244s, eta=814s\n",
      "Iteration 240, Loss: -2.760817, T=260s, eta=821s\n",
      "Iteration 250, Loss: -2.759989, T=276s, eta=825s\n",
      "Iteration 260, Loss: -2.768184, T=291s, eta=828s\n",
      "Iteration 270, Loss: -2.771917, T=308s, eta=831s\n",
      "Iteration 280, Loss: -2.754608, T=324s, eta=832s\n",
      "Iteration 290, Loss: -2.772690, T=340s, eta=831s\n",
      "Iteration 300, Loss: -2.770548, T=356s, eta=830s\n",
      "Iteration 310, Loss: -2.773933, T=372s, eta=827s\n",
      "Iteration 320, Loss: -2.772427, T=388s, eta=823s\n",
      "Iteration 330, Loss: -2.771528, T=404s, eta=819s\n",
      "Iteration 340, Loss: -2.771430, T=420s, eta=814s\n",
      "Iteration 350, Loss: -1.800505, T=436s, eta=809s\n",
      "Iteration 360, Loss: -2.772226, T=452s, eta=802s\n",
      "Iteration 370, Loss: -2.763163, T=461s, eta=784s\n",
      "Iteration 380, Loss: -2.774585, T=470s, eta=766s\n",
      "Iteration 390, Loss: -2.772831, T=478s, eta=748s\n",
      "Iteration 400, Loss: -2.774641, T=487s, eta=730s\n",
      "Iteration 410, Loss: -2.774895, T=496s, eta=713s\n",
      "Iteration 420, Loss: -2.771468, T=505s, eta=697s\n",
      "Iteration 430, Loss: -2.769774, T=513s, eta=680s\n",
      "Iteration 440, Loss: -2.770365, T=522s, eta=664s\n",
      "Iteration 450, Loss: -2.772948, T=531s, eta=649s\n",
      "Iteration 460, Loss: -2.290385, T=540s, eta=634s\n",
      "Iteration 470, Loss: -2.776484, T=549s, eta=619s\n",
      "Iteration 480, Loss: -2.773747, T=557s, eta=604s\n",
      "Iteration 490, Loss: -2.775645, T=566s, eta=589s\n",
      "Iteration 500, Loss: -2.296445, T=575s, eta=575s\n",
      "Iteration 510, Loss: -2.322733, T=583s, eta=560s\n",
      "Iteration 520, Loss: -2.772713, T=592s, eta=546s\n",
      "Iteration 530, Loss: -2.774586, T=601s, eta=533s\n",
      "Iteration 540, Loss: -2.773482, T=609s, eta=519s\n",
      "Iteration 550, Loss: -2.322015, T=618s, eta=506s\n",
      "Iteration 560, Loss: -2.775152, T=628s, eta=493s\n",
      "Iteration 570, Loss: -2.431559, T=638s, eta=481s\n",
      "Iteration 580, Loss: -2.775151, T=648s, eta=469s\n",
      "Iteration 590, Loss: -2.774686, T=657s, eta=457s\n",
      "Iteration 600, Loss: -2.776300, T=666s, eta=444s\n",
      "Iteration 610, Loss: -2.775591, T=676s, eta=433s\n",
      "Iteration 620, Loss: -2.774982, T=686s, eta=421s\n",
      "Iteration 630, Loss: -2.775201, T=696s, eta=409s\n",
      "Iteration 640, Loss: -2.776729, T=705s, eta=397s\n",
      "Iteration 650, Loss: -2.295826, T=714s, eta=385s\n",
      "Iteration 660, Loss: -2.775952, T=723s, eta=373s\n",
      "Iteration 670, Loss: -2.327173, T=733s, eta=361s\n",
      "Iteration 680, Loss: -2.293957, T=742s, eta=350s\n",
      "Iteration 690, Loss: -2.776080, T=751s, eta=338s\n",
      "Iteration 700, Loss: -2.771129, T=761s, eta=327s\n",
      "Iteration 710, Loss: -2.776522, T=771s, eta=315s\n",
      "Iteration 720, Loss: -2.776233, T=780s, eta=304s\n",
      "Iteration 730, Loss: -2.775875, T=789s, eta=292s\n",
      "Iteration 740, Loss: -2.776686, T=799s, eta=281s\n",
      "Iteration 750, Loss: -2.291204, T=808s, eta=270s\n",
      "Iteration 760, Loss: -2.777119, T=818s, eta=259s\n",
      "Iteration 770, Loss: -2.777401, T=827s, eta=247s\n",
      "Iteration 780, Loss: -2.776048, T=836s, eta=236s\n",
      "Iteration 790, Loss: -2.776677, T=845s, eta=225s\n",
      "Iteration 800, Loss: -2.776768, T=854s, eta=214s\n",
      "Iteration 810, Loss: -2.776924, T=864s, eta=203s\n",
      "Iteration 820, Loss: -2.776991, T=873s, eta=192s\n",
      "Iteration 830, Loss: -2.777617, T=886s, eta=182s\n",
      "Iteration 840, Loss: -2.776462, T=903s, eta=173s\n",
      "Iteration 850, Loss: -2.776272, T=919s, eta=163s\n",
      "Iteration 860, Loss: -2.289957, T=936s, eta=153s\n",
      "Iteration 870, Loss: -2.777192, T=952s, eta=143s\n",
      "Iteration 880, Loss: -2.777373, T=969s, eta=133s\n",
      "Iteration 890, Loss: -2.776842, T=986s, eta=122s\n",
      "Iteration 900, Loss: -2.288962, T=1003s, eta=112s\n",
      "Iteration 910, Loss: -2.776966, T=1018s, eta=101s\n",
      "Iteration 920, Loss: -2.287553, T=1034s, eta=91s\n",
      "Iteration 930, Loss: -2.776365, T=1050s, eta=80s\n",
      "Iteration 940, Loss: -2.776595, T=1066s, eta=69s\n",
      "Iteration 950, Loss: -2.776396, T=1082s, eta=58s\n",
      "Iteration 960, Loss: -2.777479, T=1098s, eta=46s\n",
      "Iteration 970, Loss: -2.777326, T=1114s, eta=35s\n",
      "Iteration 980, Loss: -2.777361, T=1130s, eta=24s\n",
      "Iteration 990, Loss: -2.777119, T=1138s, eta=12s\n"
     ]
    }
   ],
   "source": [
    "for num_time_steps, lut_depth, reward_weights in experiments:\n",
    "    print(f\"Evaluating num_time_steps={num_time_steps}, lut_depth={lut_depth}, reward_weights={reward_weights}\")\n",
    "    print(\"Training LUT\")\n",
    "\n",
    "    system_params = initialize_system_params(jax.random.PRNGKey(0))\n",
    "    weights_str = \"\".join([str(w) for w in reward_weights])\n",
    "\n",
    "    # Train LUT\n",
    "    result = optimize_pulse(\n",
    "        U_0=rho_target,\n",
    "        C_target=rho_target,\n",
    "        system_params=system_params,\n",
    "        num_time_steps=num_time_steps,\n",
    "        lut_depth=lut_depth,\n",
    "        reward_weights=reward_weights,\n",
    "        mode=\"lookup\",\n",
    "        goal=\"fidelity\",\n",
    "        max_iter=N_training_iterations,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        learning_rate=learning_rate,\n",
    "        evo_type=\"density\",\n",
    "        batch_size=16,\n",
    "        progress=True,\n",
    "    )\n",
    "\n",
    "    with open(f\"./optimized_architectures/lut_t={num_time_steps}_l={lut_depth}_s={1}_w={weights_str}.json\", \"w\") as f:\n",
    "        json.dump(FgResult_to_dict(result), f)\n",
    "\n",
    "    result = evaluate_on_longer_time(\n",
    "        U_0 = rho_target,\n",
    "        C_target = rho_target,\n",
    "        system_params = system_params,\n",
    "        optimized_trainable_parameters = result.optimized_trainable_parameters,\n",
    "        num_time_steps = 1000,\n",
    "        evo_type = \"density\",\n",
    "        goal = \"fidelity\",\n",
    "        eval_batch_size = 16, # Default is 10\n",
    "        mode = \"lookup\",\n",
    "        rnn = None,\n",
    "        rnn_hidden_size = 30,\n",
    "    )\n",
    "\n",
    "    fidelities_lut = result.fidelity_each_timestep\n",
    "\n",
    "    jnp.savez(f\"./evaluation_results/lut_t={num_time_steps}_l={lut_depth}_s={1}_w={weights_str}.npz\", fidelities_lut=jnp.array(fidelities_lut))\n",
    "\n",
    "    # Check if RNN has to be trained (only if num_time_steps and weight are different)\n",
    "    if os.path.exists(f\"./optimized_architectures/rnn_t={num_time_steps}_s={1}_w={weights_str}.json\"):\n",
    "        print(f\"RNN for t={num_time_steps} and weights={weights_str} already trained, skipping training.\")\n",
    "        continue\n",
    "    \n",
    "    # Train RNN\n",
    "    print(\"Training RNN\")\n",
    "    result = optimize_pulse(\n",
    "        U_0=rho_target,\n",
    "        C_target=rho_target,\n",
    "        system_params=system_params,\n",
    "        num_time_steps=num_time_steps,\n",
    "        reward_weights=reward_weights,\n",
    "        mode=\"nn\",\n",
    "        goal=\"fidelity\",\n",
    "        max_iter=N_training_iterations,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        learning_rate=learning_rate,\n",
    "        evo_type=\"density\",\n",
    "        batch_size=16,\n",
    "        rnn=RNN,\n",
    "        rnn_hidden_size=30,\n",
    "        progress=True,\n",
    "    )\n",
    "\n",
    "    with open(f\"./optimized_architectures/rnn_t={num_time_steps}_s={1}_w={weights_str}.json\", \"w\") as f:\n",
    "        json.dump(FgResult_to_dict(result), f)\n",
    "\n",
    "    result = evaluate_on_longer_time(\n",
    "        U_0 = rho_target,\n",
    "        C_target = rho_target,\n",
    "        system_params = system_params,\n",
    "        optimized_trainable_parameters = result.optimized_trainable_parameters,\n",
    "        num_time_steps = 1000,\n",
    "        evo_type = \"density\",\n",
    "        goal = \"fidelity\",\n",
    "        eval_batch_size = 16, # Default is 10\n",
    "        mode = \"nn\",\n",
    "        rnn = RNN,\n",
    "        rnn_hidden_size = 30,\n",
    "    )\n",
    "\n",
    "    fidelities_rnn = result.fidelity_each_timestep\n",
    "\n",
    "    jnp.savez(f\"./evaluation_results/rnn_t={num_time_steps}_s={1}_w={weights_str}.npz\", fidelities_rnn=jnp.array(fidelities_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6e59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
