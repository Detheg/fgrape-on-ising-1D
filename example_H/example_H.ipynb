{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16070338",
   "metadata": {},
   "source": [
    "# E: State stabilization with SNAP gates and displacement gates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e400e",
   "metadata": {},
   "source": [
    "The use of feedback GRAPE applied to the Jaynes-\n",
    "Cummings scenario allows us to discover strategies\n",
    "extending the lifetime of a range of quantum states. How-\n",
    "ever, for more complex quantum states such as kitten\n",
    "states, the infidelity becomes significant after just a few\n",
    "dissipative evolution steps in spite of the feedback [cf.\n",
    "Fig. 6(c)]. This raises the question of whether the limited\n",
    "quality of the stabilization is to be attributed to a failure\n",
    "of our feedback-GRAPE learning algorithm to properly\n",
    "explore the control-parameter landscape or, rather, to the\n",
    "limited expressivity of the controls. With the goal of\n",
    "addressing this question, we test our method on the state-\n",
    "stabilization task using a more expressive control scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6003222d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"./../feedback-grape\"))\n",
    "sys.path.append(os.path.abspath(\"./../\"))\n",
    "\n",
    "# ruff: noqa\n",
    "from feedback_grape.fgrape import optimize_pulse, Decay, Gate, evaluate_on_longer_time\n",
    "from feedback_grape.utils.operators import cosm, sinm, identity\n",
    "from feedback_grape.utils.states import coherent\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from feedback_grape.utils.operators import create, destroy\n",
    "from feedback_grape.utils.fidelity import ket2dm\n",
    "from library.utils.FgResult_to_dict import FgResult_to_dict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851f72b",
   "metadata": {},
   "source": [
    "## Initialize states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa672e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "N_training_iterations = 1000 # Number of training iterations\n",
    "learning_rate = 0.001 # Learning rate\n",
    "convergence_threshold = 1e-6 # Convergence threshold for early stopping\n",
    "\n",
    "#num_time_steps : Number of time steps in the control pulse\n",
    "#lut_depth : Depth of the lookup table for feedback\n",
    "#reward_weights: Weights for the reward at each time step. Default only weights last timestep [0, 0, ... 0, 1]\n",
    "\n",
    "samples = 5\n",
    "experiments = [ # (timesteps, lut_depth, reward_weights)\n",
    "    #(1,1,[1]),\n",
    "    #(2,1,[1,1]),\n",
    "    #(2,2,[0,1]),\n",
    "    #(2,2,[1,1]),\n",
    "    #(3,3,[0,0,1]),\n",
    "    (5,3,[1,1,1,1,1]),\n",
    "    #(4,4,[0,0,0,1]),\n",
    "    #(4,3,[0,0,0,1]),\n",
    "    #(4,3,[0,0,1,1]),\n",
    "    #(4,3,[1,1,1,1]),\n",
    "    #(4,4,[1,1,1,1]),\n",
    "]\n",
    "\n",
    "# Physical parameters\n",
    "N_cav = 30  # number of cavity modes\n",
    "N_snap = 15\n",
    "\n",
    "alpha = 2\n",
    "psi_target = coherent(N_cav, alpha) + coherent(N_cav, -alpha)\n",
    "\n",
    "# Normalize psi_target before constructing rho_target\n",
    "psi_target = psi_target / jnp.linalg.norm(psi_target)\n",
    "\n",
    "rho_target = ket2dm(psi_target)\n",
    "\n",
    "for _, _, weights in experiments:\n",
    "    for w in weights:\n",
    "        assert type(w) == int and 0 <= w <= 9, \"Weights must be integers between 0 and 9 to save as single character in filename.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda36a",
   "metadata": {},
   "source": [
    "## Initialize the parameterized Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9381af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displacement_gate(alphas):\n",
    "    \"\"\"Displacement operator for a coherent state.\"\"\"\n",
    "    alpha_re, alpha_im = alphas\n",
    "    alpha = alpha_re + 1j * alpha_im\n",
    "    gate = jax.scipy.linalg.expm(\n",
    "        alpha * create(N_cav) - alpha.conj() * destroy(N_cav)\n",
    "    )\n",
    "    return gate\n",
    "\n",
    "def initialize_displacement_gate(key):\n",
    "    return Gate(\n",
    "        gate=displacement_gate,\n",
    "        initial_params=jax.random.uniform(\n",
    "            key,\n",
    "            shape=(2,),\n",
    "            minval=-jnp.pi / 2,\n",
    "            maxval=jnp.pi / 2,\n",
    "            dtype=jnp.float64,\n",
    "        ),\n",
    "        measurement_flag=False,\n",
    "    )\n",
    "\n",
    "def displacement_gate_dag(alphas):\n",
    "    \"\"\"Displacement operator for a coherent state.\"\"\"\n",
    "    return displacement_gate(alphas).conj().T\n",
    "\n",
    "def initialize_displacement_gate_dag(key):\n",
    "    return Gate(\n",
    "        gate=displacement_gate_dag,\n",
    "        initial_params=jax.random.uniform(\n",
    "            key,\n",
    "            shape=(2,),\n",
    "            minval=-jnp.pi / 2,\n",
    "            maxval=jnp.pi / 2,\n",
    "            dtype=jnp.float64,\n",
    "        ),\n",
    "        measurement_flag=False,\n",
    "    )\n",
    "\n",
    "def snap_gate(phase_list):\n",
    "    diags = jnp.ones(shape=(N_cav - len(phase_list)))\n",
    "    exponentiated = jnp.exp(1j * jnp.array(phase_list))\n",
    "    diags = jnp.concatenate((exponentiated, diags))\n",
    "    return jnp.diag(diags)\n",
    "\n",
    "def initialize_snap_gate(key):\n",
    "    return Gate(\n",
    "        gate=snap_gate,\n",
    "        initial_params=jax.random.uniform(\n",
    "            key,\n",
    "            shape=(N_snap,),\n",
    "            minval=-jnp.pi / 2,\n",
    "            maxval=jnp.pi / 2,\n",
    "            dtype=jnp.float64,\n",
    "        ),\n",
    "        measurement_flag=False,\n",
    "    )\n",
    "\n",
    "def povm_measure_operator(measurement_outcome, params):\n",
    "    \"\"\"\n",
    "    POVM for the measurement of the cavity state.\n",
    "    returns Mm ( NOT the POVM element Em = Mm_dag @ Mm ), given measurement_outcome m, gamma and delta\n",
    "    \"\"\"\n",
    "    gamma, delta = params\n",
    "    cav_operator = gamma * create(N_cav) @ destroy(N_cav) + delta * identity(N_cav) / 2\n",
    "    angle = cav_operator\n",
    "    meas_op = jnp.where(\n",
    "        measurement_outcome == 1,\n",
    "        cosm(angle),\n",
    "        sinm(angle),\n",
    "    )\n",
    "    return meas_op\n",
    "\n",
    "def initialize_povm_gate(key):\n",
    "    return Gate(\n",
    "        gate=povm_measure_operator,\n",
    "        initial_params=jax.random.uniform(\n",
    "            key,\n",
    "            shape=(2,),  # 2 for gamma and delta\n",
    "            minval=-jnp.pi / 2,\n",
    "            maxval=jnp.pi / 2,\n",
    "            dtype=jnp.float64,\n",
    "        ),\n",
    "        measurement_flag=True,\n",
    "    )\n",
    "\n",
    "decay_gate = Decay(c_ops=[jnp.sqrt(0.005) * destroy(N_cav)])\n",
    "\n",
    "def initialize_system_params(key):\n",
    "    keys = jax.random.split(key, 4)\n",
    "    return [\n",
    "        decay_gate,\n",
    "        initialize_povm_gate(keys[0]),\n",
    "        decay_gate,\n",
    "        initialize_displacement_gate(keys[1]),\n",
    "        initialize_snap_gate(keys[2]),\n",
    "        initialize_displacement_gate_dag(keys[3])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d72e6",
   "metadata": {},
   "source": [
    "## Initialize RNN of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83584086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "# You can do whatever you want inside so long as you maintaing the hidden_size and output size shapes\n",
    "class RNN(nn.Module):\n",
    "    hidden_size: int  # number of features in the hidden state\n",
    "    output_size: int  # number of features in the output (inferred from the number of parameters) just provide those attributes to the class\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, measurement, hidden_state):\n",
    "\n",
    "        if measurement.ndim == 1:\n",
    "            measurement = measurement.reshape(1, -1)\n",
    "\n",
    "        ###############\n",
    "        ### Free to change whatever you want below as long as hidden layers have size self.hidden_size\n",
    "        ### and output layer has size self.output_size\n",
    "        ###############\n",
    "\n",
    "        gru_cell = nn.GRUCell(\n",
    "            features=self.hidden_size,\n",
    "            gate_fn=nn.sigmoid,\n",
    "            activation_fn=nn.tanh,\n",
    "        )\n",
    "        self.make_rng('dropout')\n",
    "\n",
    "        new_hidden_state, _ = gru_cell(hidden_state, measurement)\n",
    "        new_hidden_state = nn.Dropout(rate=0.1, deterministic=False)(\n",
    "            new_hidden_state\n",
    "        )\n",
    "        # this returns the povm_params after linear regression through the hidden state which contains\n",
    "        # the information of the previous time steps and this is optimized to output best povm_params\n",
    "        # new_hidden_state = nn.Dense(features=self.hidden_size)(new_hidden_state)\n",
    "        new_hidden_state = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "        )(new_hidden_state)\n",
    "        new_hidden_state = nn.relu(new_hidden_state)\n",
    "        new_hidden_state = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "        )(new_hidden_state)\n",
    "        new_hidden_state = nn.relu(new_hidden_state)\n",
    "        output = nn.Dense(\n",
    "            features=self.output_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "            bias_init=nn.initializers.constant(0.1),\n",
    "        )(new_hidden_state)\n",
    "        output = nn.relu(output)\n",
    "\n",
    "        ###############\n",
    "        ### Do not change the return statement\n",
    "        ###############\n",
    "\n",
    "        return output[0], new_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b486ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating num_time_steps=5, lut_depth=3, reward_weights=[1, 1, 1, 1, 1]\n",
      "Training LUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 22:00:44.961477: E external/xla/xla/service/slow_operation_alarm.cc:73] \n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2025-11-10 22:00:54.322421: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 2m9.368274s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Loss: 0.160934, T=14s, eta=1265s\n",
      "Iteration 20, Loss: 0.172045, T=27s, eta=1306s\n",
      "Iteration 30, Loss: 0.195961, T=41s, eta=1311s\n",
      "Iteration 40, Loss: 0.226193, T=55s, eta=1306s\n",
      "Iteration 50, Loss: 0.291703, T=69s, eta=1299s\n",
      "Iteration 60, Loss: 0.299828, T=83s, eta=1289s\n",
      "Iteration 70, Loss: 0.270495, T=97s, eta=1278s\n",
      "Iteration 80, Loss: 0.267050, T=112s, eta=1276s\n",
      "Iteration 90, Loss: 0.234718, T=126s, eta=1266s\n",
      "Iteration 100, Loss: 0.376861, T=140s, eta=1255s\n",
      "Iteration 110, Loss: 0.317653, T=154s, eta=1243s\n",
      "Iteration 120, Loss: 0.346998, T=169s, eta=1231s\n",
      "Iteration 130, Loss: 0.327637, T=183s, eta=1219s\n",
      "Iteration 140, Loss: 0.334984, T=197s, eta=1206s\n",
      "Iteration 150, Loss: 0.379662, T=211s, eta=1193s\n",
      "Iteration 160, Loss: 0.362941, T=226s, eta=1180s\n",
      "Iteration 170, Loss: 0.248496, T=240s, eta=1167s\n",
      "Iteration 180, Loss: 0.328306, T=254s, eta=1154s\n",
      "Iteration 190, Loss: 0.393582, T=268s, eta=1140s\n",
      "Iteration 200, Loss: 0.279497, T=282s, eta=1127s\n",
      "Iteration 210, Loss: 0.454245, T=297s, eta=1113s\n",
      "Iteration 220, Loss: 0.299311, T=311s, eta=1100s\n",
      "Iteration 230, Loss: 0.387352, T=326s, eta=1088s\n",
      "Iteration 240, Loss: 0.247998, T=341s, eta=1078s\n",
      "Iteration 250, Loss: 0.409206, T=355s, eta=1065s\n",
      "Iteration 260, Loss: 0.300306, T=370s, eta=1050s\n",
      "Iteration 270, Loss: 0.439625, T=385s, eta=1038s\n",
      "Iteration 280, Loss: 0.436598, T=399s, eta=1025s\n",
      "Iteration 290, Loss: 0.433997, T=414s, eta=1012s\n",
      "Iteration 300, Loss: 0.389224, T=428s, eta=998s\n",
      "Iteration 310, Loss: 0.519931, T=443s, eta=986s\n",
      "Iteration 320, Loss: 0.321107, T=458s, eta=972s\n",
      "Iteration 330, Loss: 0.439546, T=472s, eta=958s\n",
      "Iteration 340, Loss: 0.369812, T=486s, eta=943s\n",
      "Iteration 350, Loss: 0.295047, T=501s, eta=929s\n",
      "Iteration 360, Loss: 0.304549, T=515s, eta=914s\n",
      "Iteration 370, Loss: 0.420648, T=529s, eta=899s\n",
      "Iteration 380, Loss: 0.446676, T=543s, eta=885s\n",
      "Iteration 390, Loss: 0.227963, T=557s, eta=870s\n",
      "Iteration 400, Loss: 0.277439, T=571s, eta=856s\n",
      "Iteration 410, Loss: 0.745535, T=585s, eta=841s\n",
      "Iteration 420, Loss: 0.110982, T=599s, eta=827s\n",
      "Iteration 430, Loss: 0.109413, T=613s, eta=813s\n",
      "Iteration 440, Loss: 0.391105, T=627s, eta=798s\n",
      "Iteration 450, Loss: 0.205713, T=642s, eta=784s\n",
      "Iteration 460, Loss: 0.255519, T=656s, eta=770s\n",
      "Iteration 470, Loss: 0.395187, T=671s, eta=756s\n",
      "Iteration 480, Loss: 0.091632, T=685s, eta=742s\n",
      "Iteration 490, Loss: -0.089010, T=699s, eta=728s\n",
      "Iteration 500, Loss: 0.150188, T=714s, eta=714s\n",
      "Iteration 510, Loss: 0.047465, T=728s, eta=699s\n",
      "Iteration 520, Loss: -0.077244, T=742s, eta=685s\n",
      "Iteration 530, Loss: -0.052873, T=756s, eta=671s\n",
      "Iteration 540, Loss: -0.113786, T=771s, eta=656s\n",
      "Iteration 550, Loss: 0.222348, T=785s, eta=642s\n",
      "Iteration 560, Loss: -0.177026, T=799s, eta=628s\n",
      "Iteration 570, Loss: -0.233912, T=814s, eta=614s\n",
      "Iteration 580, Loss: -0.363929, T=828s, eta=600s\n",
      "Iteration 590, Loss: -0.358111, T=842s, eta=586s\n",
      "Iteration 600, Loss: -0.173843, T=857s, eta=571s\n",
      "Iteration 610, Loss: -0.426935, T=871s, eta=557s\n",
      "Iteration 620, Loss: 0.029987, T=885s, eta=543s\n",
      "Iteration 630, Loss: -0.364916, T=900s, eta=529s\n",
      "Iteration 640, Loss: -0.438369, T=914s, eta=514s\n",
      "Iteration 650, Loss: -0.315371, T=928s, eta=500s\n",
      "Iteration 660, Loss: -0.357604, T=942s, eta=486s\n",
      "Iteration 670, Loss: -0.214955, T=956s, eta=471s\n",
      "Iteration 680, Loss: -0.462983, T=970s, eta=457s\n",
      "Iteration 690, Loss: -0.439578, T=985s, eta=443s\n",
      "Iteration 700, Loss: -0.361359, T=999s, eta=429s\n",
      "Iteration 710, Loss: -0.515043, T=1013s, eta=414s\n",
      "Iteration 720, Loss: -0.485322, T=1027s, eta=400s\n",
      "Iteration 730, Loss: -0.154127, T=1041s, eta=386s\n",
      "Iteration 740, Loss: -0.486550, T=1055s, eta=371s\n",
      "Iteration 750, Loss: -0.452734, T=1069s, eta=357s\n",
      "Iteration 760, Loss: -0.477679, T=1084s, eta=343s\n",
      "Iteration 770, Loss: -0.516551, T=1098s, eta=329s\n",
      "Iteration 780, Loss: -0.512553, T=1112s, eta=314s\n",
      "Iteration 790, Loss: -0.318654, T=1126s, eta=300s\n",
      "Iteration 800, Loss: -0.399583, T=1140s, eta=286s\n",
      "Iteration 810, Loss: -0.477241, T=1154s, eta=271s\n",
      "Iteration 820, Loss: -0.562955, T=1168s, eta=257s\n",
      "Iteration 830, Loss: -0.387696, T=1183s, eta=243s\n",
      "Iteration 840, Loss: -0.404507, T=1197s, eta=229s\n",
      "Iteration 850, Loss: -0.475236, T=1211s, eta=214s\n",
      "Iteration 860, Loss: -0.235202, T=1225s, eta=200s\n",
      "Iteration 870, Loss: -0.330299, T=1239s, eta=186s\n",
      "Iteration 880, Loss: -0.118892, T=1253s, eta=172s\n",
      "Iteration 890, Loss: -0.397235, T=1267s, eta=157s\n",
      "Iteration 900, Loss: -0.269123, T=1282s, eta=143s\n",
      "Iteration 910, Loss: -0.233577, T=1296s, eta=129s\n",
      "Iteration 920, Loss: -0.403670, T=1310s, eta=115s\n",
      "Iteration 930, Loss: -0.578423, T=1324s, eta=101s\n",
      "Iteration 940, Loss: -0.344113, T=1338s, eta=86s\n",
      "Iteration 950, Loss: -0.689438, T=1352s, eta=72s\n",
      "Iteration 960, Loss: -0.242145, T=1367s, eta=58s\n",
      "Iteration 970, Loss: -0.726627, T=1381s, eta=44s\n",
      "Iteration 980, Loss: -0.721050, T=1395s, eta=29s\n",
      "Iteration 990, Loss: -0.727121, T=1409s, eta=15s\n"
     ]
    }
   ],
   "source": [
    "for num_time_steps, lut_depth, reward_weights in experiments:\n",
    "    for s in range(samples):\n",
    "        print(f\"Evaluating num_time_steps={num_time_steps}, lut_depth={lut_depth}, reward_weights={reward_weights}\")\n",
    "        print(\"Training LUT\")\n",
    "\n",
    "        system_params = initialize_system_params(jax.random.PRNGKey(s))\n",
    "        weights_str = \"\".join([str(w) for w in reward_weights])\n",
    "\n",
    "        # Train LUT\n",
    "        result = optimize_pulse(\n",
    "            U_0=rho_target,\n",
    "            C_target=rho_target,\n",
    "            system_params=system_params,\n",
    "            num_time_steps=num_time_steps,\n",
    "            lut_depth=lut_depth,\n",
    "            reward_weights=reward_weights,\n",
    "            mode=\"lookup\",\n",
    "            goal=\"fidelity\",\n",
    "            max_iter=N_training_iterations,\n",
    "            convergence_threshold=convergence_threshold,\n",
    "            learning_rate=learning_rate,\n",
    "            evo_type=\"density\",\n",
    "            batch_size=16,\n",
    "            progress=True,\n",
    "        )\n",
    "\n",
    "        with open(f\"./optimized_architectures/lut_t={num_time_steps}_l={lut_depth}_s={s}_w={weights_str}.json\", \"w\") as f:\n",
    "            json.dump(FgResult_to_dict(result), f)\n",
    "\n",
    "        result = evaluate_on_longer_time(\n",
    "            U_0 = rho_target,\n",
    "            C_target = rho_target,\n",
    "            system_params = system_params,\n",
    "            optimized_trainable_parameters = result.optimized_trainable_parameters,\n",
    "            num_time_steps = 1000,\n",
    "            evo_type = \"density\",\n",
    "            goal = \"fidelity\",\n",
    "            eval_batch_size = 16, # Default is 10\n",
    "            mode = \"lookup\",\n",
    "            rnn = None,\n",
    "            rnn_hidden_size = 30,\n",
    "        )\n",
    "\n",
    "        fidelities_lut = result.fidelity_each_timestep\n",
    "\n",
    "        jnp.savez(f\"./evaluation_results/lut_t={num_time_steps}_l={lut_depth}_s={s}_w={weights_str}.npz\", fidelities_lut=jnp.array(fidelities_lut))\n",
    "\n",
    "        # Check if RNN has to be trained (only if num_time_steps and weight are different)\n",
    "        if os.path.exists(f\"./optimized_architectures/rnn_t={num_time_steps}_s={s}_w={weights_str}.json\"):\n",
    "            print(f\"RNN for t={num_time_steps} and weights={weights_str} already trained, skipping training.\")\n",
    "            continue\n",
    "        \"\"\"\n",
    "        # Train RNN\n",
    "        print(\"Training RNN\")\n",
    "        result = optimize_pulse(\n",
    "            U_0=rho_target,\n",
    "            C_target=rho_target,\n",
    "            system_params=system_params,\n",
    "            num_time_steps=num_time_steps,\n",
    "            reward_weights=reward_weights,\n",
    "            mode=\"nn\",\n",
    "            goal=\"fidelity\",\n",
    "            max_iter=N_training_iterations,\n",
    "            convergence_threshold=convergence_threshold,\n",
    "            learning_rate=learning_rate,\n",
    "            evo_type=\"density\",\n",
    "            batch_size=16,\n",
    "            rnn=RNN,\n",
    "            rnn_hidden_size=30,\n",
    "            progress=True,\n",
    "        )\n",
    "\n",
    "        with open(f\"./optimized_architectures/rnn_t={num_time_steps}_s={s}_w={weights_str}.json\", \"w\") as f:\n",
    "            json.dump(FgResult_to_dict(result), f)\n",
    "\n",
    "        result = evaluate_on_longer_time(\n",
    "            U_0 = rho_target,\n",
    "            C_target = rho_target,\n",
    "            system_params = system_params,\n",
    "            optimized_trainable_parameters = result.optimized_trainable_parameters,\n",
    "            num_time_steps = 1000,\n",
    "            evo_type = \"density\",\n",
    "            goal = \"fidelity\",\n",
    "            eval_batch_size = 16, # Default is 10\n",
    "            mode = \"nn\",\n",
    "            rnn = RNN,\n",
    "            rnn_hidden_size = 30,\n",
    "        )\n",
    "\n",
    "        fidelities_rnn = result.fidelity_each_timestep\n",
    "\n",
    "        jnp.savez(f\"./evaluation_results/rnn_t={num_time_steps}_s={s}_w={weights_str}.npz\", fidelities_rnn=jnp.array(fidelities_rnn))\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47e17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
